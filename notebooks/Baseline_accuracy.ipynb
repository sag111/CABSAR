{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6150adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6544bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    res = []\n",
    "    f = open(path, \"r\")\n",
    "    for line in f:\n",
    "        res.append(json.loads(line))\n",
    "    f.close()\n",
    "    return res\n",
    "    \n",
    "def load_model(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029d602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "train_df = load_data('../data/train.jsonl')\n",
    "test_df = load_data('../data/test.jsonl')\n",
    "\n",
    "target_names = {0: 'neutral', 1: 'positive', 2: 'negative'}\n",
    "\n",
    "with open('../data/sentiment_lexicon.json') as f:\n",
    "    sentiment_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e0feb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random approach:\n",
      "Test source name: twitter\t, F1-mic.: 0.34;\t F1-mac.:0.33\n",
      "Test source name: lj\t, F1-mic.: 0.34;\t F1-mac.:0.33\n",
      "Test source name: lenta\t, F1-mic.: 0.34;\t F1-mac.:0.32\n",
      "Test source name: all\t, F1-mic.: 0.31;\t F1-mac.:0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.42      0.30      0.35       614\n",
      "    positive       0.32      0.33      0.33       460\n",
      "    negative       0.17      0.27      0.21       267\n",
      "\n",
      "    accuracy                           0.31      1341\n",
      "   macro avg       0.30      0.30      0.30      1341\n",
      "weighted avg       0.34      0.31      0.31      1341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random approach\n",
    "\n",
    "print('Random approach:')\n",
    "\n",
    "for test_source_name in ['twitter', 'lj', 'lenta', 'all',]:\n",
    "    true_y = []    \n",
    "    for sample in test_df:\n",
    "        if test_source_name != sample['source'] and test_source_name != 'all':\n",
    "            continue\n",
    "        \n",
    "        true_y.append(sample['label'])\n",
    "\n",
    "    # The sentiment label is chosen randomly for each aspect\n",
    "    pred_y = np.random.randint(0, 3, len(true_y))\n",
    "\n",
    "    # The accuracy of the obtained models is measured with the F1 metric\n",
    "    p_micro, r_micro, f_micro, _ = precision_recall_fscore_support(true_y, pred_y, average=\"micro\")\n",
    "    p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(true_y, pred_y, average=\"macro\")\n",
    "\n",
    "    print(f'Test source name: {test_source_name}\\t, F1-mic.: {round(f_micro, 2)};\\t F1-mac.:{round(f_macro, 2)}')\n",
    "\n",
    "print(classification_report(true_y, pred_y, target_names=target_names.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ac44e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon approach:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksandr/anaconda3/envs/git_cabsar_v001/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test source name: twitter\t, F1-mic.: 0.48;\t F1-mac.:0.36\n",
      "Test source name: lj\t, F1-mic.: 0.42;\t F1-mac.:0.34\n",
      "Test source name: lenta\t, F1-mic.: 0.35;\t F1-mac.:0.34\n",
      "Test source name: all\t, F1-mic.: 0.41;\t F1-mac.:0.35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.25      0.00      0.00       614\n",
      "    positive       0.44      0.80      0.57       460\n",
      "    negative       0.36      0.69      0.48       267\n",
      "\n",
      "    accuracy                           0.41      1341\n",
      "   macro avg       0.35      0.50      0.35      1341\n",
      "weighted avg       0.34      0.41      0.29      1341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lexicon approach\n",
    "\n",
    "print('Lexicon approach:')\n",
    "\n",
    "for test_source_name in ['twitter', 'lj', 'lenta', 'all',]:\n",
    "    true_y, pred_y = [], []\n",
    "    \n",
    "    true_y = []    \n",
    "    for sample in test_df:\n",
    "        if test_source_name != sample['source'] and test_source_name != 'all':\n",
    "            continue\n",
    "        \n",
    "        true_y.append(sample['label'])\n",
    "        sample_lemms = [word['lemma'].lower() for sentence in sample['context']['sentences'] for word in sentence]\n",
    "        \n",
    "        vocab_labels = []\n",
    "        for word_lemm in sample_lemms:\n",
    "            for label in target_names.keys():\n",
    "                if target_names[label] == 'neutral':\n",
    "                    continue\n",
    "\n",
    "                # check the word lemma in the sentiment dictionaries\n",
    "                elif word_lemm in sentiment_dict[target_names[label]]:\n",
    "                    vocab_labels.append(label)\n",
    "\n",
    "        count_sent = Counter(vocab_labels)\n",
    "        if vocab_labels == []:\n",
    "            pred_y.append(0)\n",
    "        elif count_sent[1] >= count_sent[2]:\n",
    "            pred_y.append(1)\n",
    "        else:\n",
    "            pred_y.append(2)\n",
    "\n",
    "    # The accuracy of the obtained models is measured with the F1 metric\n",
    "    p_micro, r_micro, f_micro, _ = precision_recall_fscore_support(true_y, pred_y, average=\"micro\")\n",
    "    p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(true_y, pred_y, average=\"macro\")\n",
    "\n",
    "    print(f'Test source name: {test_source_name}\\t, F1-mic.: {round(f_micro, 2)};\\t F1-mac.:{round(f_macro, 2)}')\n",
    "\n",
    "print(classification_report(true_y, pred_y, target_names=target_names.values(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee66f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPOT(ELMo) model:\n",
      "\n",
      "Test source name: twitter\t, F1-mic.: 0.63;\t F1-mac.:0.57\n",
      "Test source name: lj\t, F1-mic.: 0.59;\t F1-mac.:0.57\n",
      "Test source name: lenta\t, F1-mic.: 0.74;\t F1-mac.:0.71\n",
      "Test source name: all\t, F1-mic.: 0.66;\t F1-mac.:0.65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.77      0.57      0.66       614\n",
      "    positive       0.68      0.73      0.70       460\n",
      "    negative       0.51      0.73      0.60       267\n",
      "\n",
      "    accuracy                           0.66      1341\n",
      "   macro avg       0.65      0.68      0.65      1341\n",
      "weighted avg       0.68      0.66      0.66      1341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TPOT (ELMo)\n",
    "\n",
    "print('TPOT(ELMo) model:\\n')\n",
    "\n",
    "# load TPOT model\n",
    "tpot_model = load_model('../models/tpot_elmo_model.pkl')\n",
    "\n",
    "# load ELMO embedding vectors\n",
    "df = load_model('../data/elmo_vec.pkl')\n",
    "\n",
    "for test_source_name in ['twitter', 'lj', 'lenta', 'all',]:\n",
    "    test_x, test_y, pred_y = [], [], []\n",
    "    \n",
    "    for sample in df['test']:\n",
    "        if test_source_name != sample['source'] and test_source_name != 'all':\n",
    "            continue\n",
    "        \n",
    "        test_x.append(sample['vec'])\n",
    "        test_y.append(sample['label'])\n",
    "\n",
    "    pred_y = tpot_model.predict(test_x)\n",
    "\n",
    "    p_micro,r_micro,f_micro,_ = precision_recall_fscore_support(test_y, pred_y, average=\"micro\")\n",
    "    p_macro,r_macro,f_macro,_ = precision_recall_fscore_support(test_y, pred_y, average=\"macro\")\n",
    "\n",
    "    print(f'Test source name: {test_source_name}\\t, F1-mic.: {round(f_micro, 2)};\\t F1-mac.:{round(f_macro, 2)}')\n",
    "    \n",
    "print(classification_report(true_y, pred_y, target_names=target_names.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dab35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Successful complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.7.0 git_cabsar)",
   "language": "python",
   "name": "git_cabsar_v001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
